from __future__ import annotations

from typing import Any, DefaultDict, Dict, List, Mapping, Tuple, Union
from collections import defaultdict
from flagevalmm.evaluator import BaseEvaluator
from flagevalmm.registry import EVALUATORS
import re

demo_prompt_score = """# Task Overview

Your task is to evaluate if the whole model answer is semantically consistent with the ground truth answer for the same question. You will be given three pieces of information:

* Question: The question that was asked.
* Ground Truth Answer: The ideal, correct answer.
* Model Answer: The answer generated by an AI model.

Based on your evaluation, you will first provide a brief reason for your decision and then output a simple judgment: `1` for consistent or `0` for inconsistent.

---

# Evaluation Criteria

Determine if the `model answer` conveys the same essential information as the `ground truth answer`.

## When to Judge as Consistent (Judgement: 1)

An answer should be considered consistent if it is factually correct and complete according to the ground truth, even if the phrasing or format differs. This includes cases where:

* The meaning is identical: The answers use different words or sentence structures but mean the same thing.
    * *Example:*  GT: "Both statements are true" vs. Model: "True, True"
    * Reasoning: The model's answer "True, True" is a formatted but semantically identical representation of the ground truth "Both statements are true".
    * Judgement: 1
* The values are mathematically equivalent: The answers express numbers or units differently but are accurately equal.
    * *Example:* GT: "2.25" vs. Model: "\frac{{18}}{{8}}"; GT: "`{{9}}:{{4}}`" vs. Model: "`2.25`".
    * Reasoning: The model's answer, the fraction 9:4
, is mathematically equal to the ground truth decimal 2.25.
    * Judgement: 1
* There are minor variations in names or spelling: The answer refers to the same entity using a common variation.
    * *Example:* GT: "The U.S.S.R." vs. Model: "Soviet Union".
    * Reasoning: "Soviet Union" is a widely accepted and equivalent name for "The U.S.S.R.", referring to the exact same entity.
    * Judgement: 1
* The answer is semantically equivalent for the question: The answers use different words or sentence structures but mean the same thing.
    * *Example:* Question: "What is the medium of The Harbaville Triptych?" GT: "carved ivory" vs. Model: "ivory"
    * Reasoning: The model's answer "ivory" is semantically equivalent to the ground truth "carved ivory" for the question "What is the medium of The Harbaville Triptych?".
    * Judgement: 1

## When to Judge as Inconsistent (Judgement: 0)

An answer should be considered inconsistent if it is factually incorrect, incomplete, or contradictory. This includes cases where:

* It contains a factual error: The model's answer is demonstrably false.
    * *Example:* GT: "The capital of Australia is Canberra." vs. Model: "The capital of Australia is Sydney."
    * Reasoning: The model's answer contains a factual error by incorrectly identifying the capital as "Sydney" when the ground truth is "Canberra".
    * Judgement: 0
* It is missing crucial information: The model's answer omits a key part of the ground truth.
    * *Example:* GT: "The primary colors are red, yellow, and blue." vs. Model: "The primary colors are red and yellow."
    * Reasoning: The model's answer is incomplete because it omits the color "blue," which is a required component of the ground truth list.
    * Judgement: 0
* It contradicts the ground truth: The model's answer states the opposite of the ground truth.
    * *Example:* GT: "The reaction is endothermic." vs. Model: "The reaction is exothermic."
    * Reasoning: The model's answer "exothermic" is the direct opposite of and contradicts the ground truth "endothermic".
    * Judgement: 0
* It is irrelevant: The model's answer is off-topic or fails to address the question.
    * *Example:* GT: "The capital of France is Paris." vs. Model: "The capital of China is Beijing."
    * Reasoning: The model's answer is irrelevant because it provides the capital of China, while the ground truth is about the capital of France.
    * Judgement: 0
* The values are mathematically approximate but inequivalent: The answers are not accurately equal unless approximate is specified.
    * *Example:* GT: "54" vs. Model: "54.584".
    * Reasoning: The model's answer, 54.584, is not accurately equal to the ground truth 54.
    * Judgement: 0
---

# Output Format

Your response must consist of two parts in the following order:

1.  Reasoning: Your explanation for your decision.
2.  Judgement: The numerical judgment on a new line, starting with "Judgement: "

[Question]: {{question}}
[Ground Truth]: {{answer}}
[Model Response] : {{extracted_answer}}
"""


def is_chinese(text: str) -> bool:
    """Check if the text contains Chinese characters"""
    for char in text:
        if "\u4e00" <= char <= "\u9fff":
            return True
    return False


def normalize_string(text: str) -> str:
    # replace spacial characters
    replace_dict = {
        "′": "'",
        " ": " ",
        "‐": "-",
        "−": "-",
        "–": "-",
        "⋅": "·",
        "\\sim": "~",
        "\\circ": "°",
        "\\theta": "θ",
        # LaTeX function macros to plain text
        "\\sin": "sin",
        "\\cos": "cos",
        "\\tan": "tan",
        "\\log": "log",
        "\\ln": "ln",
    }
    for k, v in replace_dict.items():
        text = text.replace(k, v)
    return text


def extract_answer(pred: Mapping[str, Any]) -> str:
    # Extract content within the LAST LaTeX \\boxed{...}: slice from last opening to the last '}'
    text = str(pred.get("answer", ""))
    matches = list(re.finditer(r"\\boxed\{", text))
    if matches:
        start_idx = matches[-1].end()
        # If a closing brace is immediately followed by a newline, stop there
        close_newline_idx = text.find("}\n", start_idx)
        if close_newline_idx != -1:
            end_idx = close_newline_idx
        else:
            # Decide closing brace: simple -> first '}', complex (scientific/LaTeX/nested) -> last '}'
            first_close = text.find("}", start_idx)
            lookahead_end = (
                len(text) if first_close == -1 else min(len(text), first_close + 200)
            )
            ahead = text[start_idx:lookahead_end]
            has_sci_e = re.search(r"[+-]?\d+(?:\.\d+)?[eE][+-]?\d+", ahead) is not None
            has_times10 = re.search(r"(?:\\times|x|×)\s*10\s*\^", ahead) is not None
            has_text_macro = "\\text" in ahead
            has_open_before_first_close = (
                first_close != -1 and text.find("{", start_idx, first_close) != -1
            )
            end_idx = (
                text.rfind("}")
                if (
                    has_sci_e
                    or has_times10
                    or has_text_macro
                    or has_open_before_first_close
                )
                else first_close
            )
        if end_idx != -1 and end_idx > start_idx:
            content = text[start_idx:end_idx]
            # Remove \\text{...} while preserving inner content
            content = re.sub(r"\\text\s*\{([^}]*)\}", r"\1", content)
            # Collapse multiple whitespaces
            content = re.sub(r"\s+", " ", content).strip()
            # If content is a pure LaTeX fraction like \\frac{a}{b}, convert it to decimal
            frac_match = re.fullmatch(
                r"([+-]?)\\frac\s*\{\s*([+-]?\d+(?:\.\d+)?)\s*\}\s*\{\s*([+-]?\d+(?:\.\d+)?)\s*\}",
                content,
            )
            if frac_match:
                sign_str, num_str, den_str = (
                    frac_match.group(1),
                    frac_match.group(2),
                    frac_match.group(3),
                )
                try:
                    value = float(num_str) / float(den_str)
                    if sign_str == "-":
                        value = -value
                    content = str(value)
                except Exception:
                    pass
            return content
    indicators = ["Answer:", "Answer", "答案：", "答案:", "答案"]
    for indicator in indicators:
        if indicator in text:
            return text.split(indicator)[-1].strip()
    return text


def _extract_numbers(text: str) -> List[float]:
    """Extract numbers from text, supporting E-notation and LaTeX-style scientific notation.

    Supported formats:
    - 1.23, -4, +5.0
    - 1.23e-4, 5E+8
    - 1.23 \\times 10^{-4}, 7 x 10^{3}, 9 × 10^-2
    """

    # Pre-normalize thousands separators (e.g., "8, 000" -> "8000", "14 000" -> "14000")
    def _merge_thousands_separators(s: str) -> str:
        sep_class = r"[,\s\u00A0\u2009\u202F]"
        pattern = re.compile(
            rf"(?<!\d)([+-]?\d{{1,3}}(?:{sep_class}\d{{3}})+(?:\.\d+)?)(?!\d)"
        )

        def repl(m: re.Match[str]) -> str:
            token = m.group(1)
            token = re.sub(sep_class, "", token)
            return token

        return pattern.sub(repl, s)

    text = _merge_thousands_separators(text)

    # Combined regex: E-notation, LaTeX/typed "x 10^" notation, then simple numbers
    pattern = re.compile(
        r"""
        (?P<sci_e>[+-]?\d+(?:\.\d+)?[eE][+-]?\d+)
        |
        (?P<times>(?P<coef>[+-]?\d+(?:\.\d+)?)\s*(?:\\times|x|×)\s*10\s*\^\s*\{?\s*(?P<exp>[+-]?\d+)\s*\}?)
        |
        (?P<simple>[+-]?\d+(?:\.\d+)?)
        """,
        re.VERBOSE,
    )
    numbers: List[float] = []
    for match in pattern.finditer(text):
        sci_e = match.group("sci_e")
        if sci_e is not None:
            try:
                numbers.append(float(sci_e))
            except ValueError:
                continue
            continue
        if match.group("times") is not None:
            coef_str = match.group("coef")
            exp_str = match.group("exp")
            try:
                coef_val = float(coef_str)
                exp_val = int(exp_str)
                numbers.append(coef_val * (10**exp_val))
            except (TypeError, ValueError):
                continue
            continue
        simple = match.group("simple")
        if simple is not None:
            try:
                numbers.append(float(simple))
            except ValueError:
                continue
    return numbers


def number_matching(pred: Mapping[str, Any], value_to_match: Union[int, float]) -> int:
    # extract last numeric value, supporting scientific notation
    extracted_numbers = _extract_numbers(str(pred.get("answer", "")))
    result = extracted_numbers[-1] if extracted_numbers else None
    if result is None:
        return 0
    pred_ans = float(result)
    if isinstance(value_to_match, float):
        relative_error = abs(value_to_match) * 0.1
    else:
        relative_error = 1e-3
    return int(abs(pred_ans - value_to_match) < relative_error)


def numbers_matching(
    pred: Mapping[str, Any], values_to_match: List[Union[int, float]]
) -> int:
    """Ordered numeric list matching with per-number tolerance.

    Returns 1 if prediction contains exactly the same count of numbers
    and each number matches the reference at the same position within tolerance.
    """
    extracted_numbers = _extract_numbers(str(pred.get("answer", "")))
    if not extracted_numbers:
        return 0
    pred_numbers = [float(x) for x in extracted_numbers]

    if len(pred_numbers) != len(values_to_match):
        return 0

    def within_tol(a: float, b: Union[int, float]) -> bool:
        if isinstance(b, float) and not float(b).is_integer():
            tol = abs(float(b)) * 0.08
        else:
            tol = 1e-3
        return abs(float(a) - float(b)) < tol

    for a, b in zip(pred_numbers, values_to_match):
        if not within_tol(a, b):
            return 0
    return 1


def tf_list_matching(
    pred: Mapping[str, Any], values_to_match: List[Union[str, bool]]
) -> int:
    """Match a list of booleans extracted from text against reference booleans."""
    ref_bools: List[bool] = []
    for v in values_to_match:
        if isinstance(v, bool):
            ref_bools.append(v)
        else:
            s = str(v).strip().lower()
            if s == "true":
                ref_bools.append(True)
            elif s == "false":
                ref_bools.append(False)
            else:
                return 0

    # Inline parse_bool_tokens: extract only 'true'/'false' tokens in order
    lowered = str(pred.get("answer", "")).lower()
    tokens = re.findall(r"\b(true|false)\b", lowered)
    pred_bools: List[bool] = []
    for tok in tokens:
        if tok == "true":
            pred_bools.append(True)
        elif tok == "false":
            pred_bools.append(False)
    if len(pred_bools) != len(ref_bools):
        return 0
    return int(all(a == b for a, b in zip(pred_bools, ref_bools)))


def text_match(pred: Mapping[str, Any], candidates: List[str]) -> int:
    """Case-insensitive OR match over a list of candidate answers."""
    pred_text = str(pred.get("answer", "")).lower()
    return int(any(str(candidate).lower() in pred_text for candidate in candidates))


@EVALUATORS.register_module()
class OpenEvaluator(BaseEvaluator):
    def __init__(
        self,
        tracker_type=None,
        tracker_subtype=None,
        **kwargs,
    ):
        self.tracker_type = tracker_type
        self.tracker_subtype = tracker_subtype
        super().__init__(**kwargs)

    def get_score(self, gt: Dict[str, Any], pred: Dict[str, Any]) -> int:
        pred["raw_answer"] = str(pred.get("answer", ""))
        pred["answer"] = normalize_string(extract_answer(pred))

        # Preferred path: dispatch by answer_type if present
        answer_type = gt.get("answer_type")
        assert answer_type is not None, f"answer_type is None for gt: {gt}"
        if answer_type == "integer":
            if isinstance(gt["answer"], list):
                return numbers_matching(pred, [int(x) for x in gt["answer"]])
            return number_matching(pred, int(gt["answer"]))
        if answer_type == "float":
            if isinstance(gt["answer"], list):
                return numbers_matching(pred, [float(x) for x in gt["answer"]])
            return number_matching(pred, float(gt["answer"]))
        if answer_type == "true_false":
            return tf_list_matching(pred, gt["answer"])  # list of booleans/strings
        if answer_type == "text match":
            if not isinstance(gt["answer"], list):
                return text_match(pred, [gt["answer"]])
            else:
                return text_match(pred, gt["answer"])  # string or list of alternatives
        if answer_type == "open_ended":
            # For open-ended, use LLM-based scoring
            _, score = self.get_score_by_llm(gt, pred)
            return score

        # No legacy evaluator fallback; raise if unrecognized
        raise ValueError(
            f"Unsupported or missing answer_type with provided gt: keys={list(gt.keys())}"
        )

    def get_score_by_llm(
        self, gt: Dict[str, Any], pred: Dict[str, Any]
    ) -> Tuple[str, int]:
        """Custom grading method"""
        prompt = (
            demo_prompt_score.replace("{{question}}", gt["question"])
            .replace("{{answer}}", gt["answer"])
            .replace("{{extracted_answer}}", pred["answer"])
        )

        message = self.llm_evaluator.build_message(query=prompt)
        print(f"message: {message}")
        compare_result_response = self.llm_evaluator.infer(
            chat_messages=message, temperature=0, top_p=1, seed=42
        )
        compare_result = compare_result_response.replace("Judgement:", "").strip()
        # Extract the score from the custom evaluation result
        # The score should be after the last double newline (\n\n)
        if "\n\n" in compare_result:
            # Split by double newlines and get the last part
            parts = compare_result.split("\n\n")
            score_part = parts[-1].strip()

            # Extract numeric value from the score part
            import re

            score_match = re.search(r"(\d+(?:\.\d+)?)", score_part)
            if score_match:
                compare_result = score_match.group(1)
            else:
                # If no numeric value found, default to 0
                compare_result = "0"
        return compare_result_response, int(compare_result)

    def cal_accuracy(
        self,
        annotations: Dict[str, Dict[str, Any]],
        predictions: List[Dict[str, Any]],
        *args,
        **kwargs,
    ) -> Dict[str, Any]:
        class ScoreTracker:
            def __init__(self):
                self.total_score: int = 0
                self.count: int = 0
                self.accuracy: float = 0.0
                # [score_sum, count, accuracy] - keep as floats for simplicity
                self.subtypes: DefaultDict[str, List[float]] = defaultdict(
                    lambda: [0.0, 0.0, 0.0]
                )  # [score_sum, count, accuracy]

            def update(self, score: int, sub_type: Any) -> None:
                sub_key = str(sub_type)
                self.total_score += score
                self.count += 1
                self.subtypes[sub_key][0] += float(score)
                self.subtypes[sub_key][1] += 1.0

        results: Dict[str, Any] = {}
        scores_by_type: DefaultDict[str, ScoreTracker] = defaultdict(ScoreTracker)

        assert (
            self.tracker_type is not None
        ), "tracker_type must be set for cal_accuracy()"
        for pred in predictions:
            question_id = str(pred["question_id"])
            gt = annotations[question_id]
            # Decide LLM vs rule by answer_type
            if gt.get("answer_type") == "open_ended":
                judgement_response, score = self.get_score_by_llm(gt, pred)
            else:
                judgement_response = None
                score = self.get_score(gt, pred)
            gt["label"] = gt["answer"]
            gt.pop("answer")
            pred.update(gt)
            pred["correct"] = score
            pred["judgement_response"] = judgement_response
            # Update scores
            tracker = scores_by_type[str(pred[self.tracker_type])]
            if self.tracker_subtype is not None:
                tracker.update(score, pred[self.tracker_subtype])
            else:
                tracker.update(score, pred[self.tracker_type])
        # Calculate accuracy
        for tracker in scores_by_type.values():
            tracker.accuracy = round(tracker.total_score / tracker.count, 3)
            for sub_type in tracker.subtypes:
                tracker.subtypes[sub_type][2] = round(
                    tracker.subtypes[sub_type][0] / tracker.subtypes[sub_type][1], 3
                )
        final_score = sum(tracker.total_score for tracker in scores_by_type.values())
        results["final_score"] = [final_score, len(predictions)]
        results["accuracy"] = round((final_score / len(predictions)) * 100, 3)

        # Convert ScoreTracker objects to the expected format
        for qtype, tracker in scores_by_type.items():
            results[qtype] = [
                tracker.total_score,
                tracker.count,
                tracker.accuracy,
                dict(tracker.subtypes),
            ]

        return results
