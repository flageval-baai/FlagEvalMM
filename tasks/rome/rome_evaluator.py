from typing import Dict, List, Tuple, Union
from collections import defaultdict
from flagevalmm.evaluator import BaseEvaluator
from flagevalmm.registry import EVALUATORS
from flagevalmm.evaluator.pre_process import process_multiple_choice
import re

demo_prompt_score = """# Task Overview

Your task is to evaluate if the whole model answer is semantically consistent with the ground truth answer for the same question. You will be given three pieces of information:

* Question: The question that was asked.
* Ground Truth Answer: The ideal, correct answer.
* Model Answer: The answer generated by an AI model.

Based on your evaluation, you will first provide a brief reason for your decision and then output a simple judgment: `1` for consistent or `0` for inconsistent.

---

# Evaluation Criteria

Determine if the `model answer` conveys the same essential information as the `ground truth answer`.

## When to Judge as Consistent (Judgement: 1)

An answer should be considered consistent if it is factually correct and complete according to the ground truth, even if the phrasing or format differs. This includes cases where:

* The meaning is identical: The answers use different words or sentence structures but mean the same thing.
    * *Example:*  GT: "Both statements are true" vs. Model: "True, True"
    * Reasoning: The model's answer "True, True" is a formatted but semantically identical representation of the ground truth "Both statements are true".
    * Judgement: 1
* The values are mathematically equivalent: The answers express numbers or units differently but are accurately equal.
    * *Example:* GT: "2.25" vs. Model: "\frac{{18}}{{8}}"; GT: "`{{9}}:{{4}}`" vs. Model: "`2.25`".
    * Reasoning: The model's answer, the fraction 9:4
, is mathematically equal to the ground truth decimal 2.25.
    * Judgement: 1
* There are minor variations in names or spelling: The answer refers to the same entity using a common variation.
    * *Example:* GT: "The U.S.S.R." vs. Model: "Soviet Union".
    * Reasoning: "Soviet Union" is a widely accepted and equivalent name for "The U.S.S.R.", referring to the exact same entity.
    * Judgement: 1
* The answer is semantically equivalent for the question: The answers use different words or sentence structures but mean the same thing.
    * *Example:* Question: "What is the medium of The Harbaville Triptych?" GT: "carved ivory" vs. Model: "ivory"
    * Reasoning: The model's answer "ivory" is semantically equivalent to the ground truth "carved ivory" for the question "What is the medium of The Harbaville Triptych?".
    * Judgement: 1

## When to Judge as Inconsistent (Judgement: 0)

An answer should be considered inconsistent if it is factually incorrect, incomplete, or contradictory. This includes cases where:

* It contains a factual error: The model's answer is demonstrably false.
    * *Example:* GT: "The capital of Australia is Canberra." vs. Model: "The capital of Australia is Sydney."
    * Reasoning: The model's answer contains a factual error by incorrectly identifying the capital as "Sydney" when the ground truth is "Canberra".
    * Judgement: 0
* It is missing crucial information: The model's answer omits a key part of the ground truth.
    * *Example:* GT: "The primary colors are red, yellow, and blue." vs. Model: "The primary colors are red and yellow."
    * Reasoning: The model's answer is incomplete because it omits the color "blue," which is a required component of the ground truth list.
    * Judgement: 0
* It contradicts the ground truth: The model's answer states the opposite of the ground truth.
    * *Example:* GT: "The reaction is endothermic." vs. Model: "The reaction is exothermic."
    * Reasoning: The model's answer "exothermic" is the direct opposite of and contradicts the ground truth "endothermic".
    * Judgement: 0
* It is irrelevant: The model's answer is off-topic or fails to address the question.
    * *Example:* GT: "The capital of France is Paris." vs. Model: "The capital of China is Beijing."
    * Reasoning: The model's answer is irrelevant because it provides the capital of China, while the ground truth is about the capital of France.
    * Judgement: 0
* The values are mathematically approximate but inequivalent: The answers are not accurately equal unless approximate is specified.
    * *Example:* GT: "54" vs. Model: "54.584".
    * Reasoning: The model's answer, 54.584, is not accurately equal to the ground truth 54.
    * Judgement: 0
---

# Output Format

Your response must consist of two parts in the following order:

1.  Reasoning: Your explanation for your decision.
2.  Judgement: The numerical judgment on a new line, starting with "Judgement: "

[Question]: {{question}}
[Ground Truth]: {{answer}}
[Model Response] : {{extracted_answer}}
"""


def is_chinese(text):
    """Check if the text contains Chinese characters"""
    for char in text:
        if "\u4e00" <= char <= "\u9fff":
            return True
    return False


def normalize_string(text: str):
    # replace spacial characters
    replace_dict = {"′": "'", " ": " ", "‐": "-", "−": "-", "–": "-", "⋅": "·"}
    for k, v in replace_dict.items():
        text = text.replace(k, v)
    return text


def extract_answer(pred: Dict) -> str:
    indicators = ["Answer:", "Answer", "答案：", "答案:", "答案"]
    for indicator in indicators:
        if indicator in pred["answer"]:
            return pred["answer"].split(indicator)[-1].strip()
    boxed_pattern = r"\\boxed\{([^}]+)\}"
    boxed_match = re.search(boxed_pattern, pred["answer"])
    if boxed_match:
        return boxed_match.group(1).strip()
    return pred["answer"]


def key_items_matching(
    pred: Dict, key_items: List[List[str]], remove_space=False
) -> int:
    def process(x):
        if remove_space:
            x = x.replace(" ", "")
        return x.lower()

    # Check if any answer variant matches the prediction as a substring
    def match_any(pred_str, answer_variants):
        assert isinstance(answer_variants, list)
        return int(any(process(answer) in pred_str for answer in answer_variants))

    processed_pred = process(pred["answer"])
    if isinstance(key_items[0], list):
        return int(
            all(match_any(processed_pred, item_variants) for item_variants in key_items)
        )
    elif isinstance(key_items[0], str):
        return int(match_any(processed_pred, key_items))
    else:
        raise ValueError(f"Unsupported key_items type: {type(key_items[0])}")


def _interval_matching(
    pred: Dict, interval: List[Union[float, str]], units: List[str]
) -> int:
    def time_to_seconds(match):
        scale = [3600, 60, 1]
        seconds = 0
        for i in range(len(match)):
            if match[i] != "":
                seconds += scale[i] * int(match[i])
        return seconds

    pred_str = pred["answer"]
    # Time interval
    if isinstance(interval[0], str):
        left_interval = time_to_seconds(interval[0].split(":"))
        right_interval = time_to_seconds(interval[1].split(":"))
        time_pattern = r"\b(\d{1,2}):(\d{2})(?::(\d{2}))?\b"

        # Find all time matches in the prediction string
        matches = re.findall(time_pattern, pred_str)

        if not matches:
            return 0
        match = matches[0]
        if len(match) < 2 or len(match) > 3:
            return 0
        pred_ans = time_to_seconds(match)
    else:
        # Number interval
        left_interval = interval[0]
        right_interval = interval[1]
        result = re.search(r"-?\d+(?:\.\d+)?", pred_str)
        if result is None:
            return 0
        pred_ans = float(result.group())
    if pred_ans < left_interval or pred_ans > right_interval:
        return 0
    pred["number_correct"] = 1
    # Answer without units
    if len(units) == 0:
        return 1
    # Check if the answer contains any of the units
    for unit in units:
        if unit in pred_str:
            pred["unit_correct"] = 1
            return 1
    return 0


def interval_matching(
    pred: Dict, interval: List[Union[float, str]], units: List[str]
) -> int:
    return _interval_matching(pred, interval, units)


def multi_interval_matching(
    pred: Dict, intervals: List[List[Union[float, str]]], units: List[str]
) -> int:
    for interval, unit in zip(intervals, units):
        if _interval_matching(pred, interval, unit) == 1:
            return 1
    return 0


def choices_matching(pred: Dict, label: str) -> int:
    pred["answer"] = process_multiple_choice(pred["answer"])
    label = label.upper()
    if len(label) > 1:
        label = "".join(sorted(label))
        pred["answer"] = "".join(sorted(set(pred["answer"])))
    elif len(pred["answer"]) > 1:
        pred["answer"] = pred["answer"][0]
    return int(label == pred["answer"])


def ordered_list_matching(
    pred: Dict, order: Union[str, List[str]]
) -> int:  # Is label a subsequence of pred_ans
    if isinstance(order, list):
        order = ",".join(order)
    pred_ans = pred["answer"].lower()
    order = order.lower().replace(" ", "")

    # Use two pointers approach to check if label is a subsequence of pred_ans
    i, j = 0, 0
    while i < len(pred_ans) and j < len(order):
        if pred_ans[i] == order[j]:
            j += 1
        i += 1

    return int(j == len(order))


def number_matching(pred: Dict, value_to_match: Union[int, float]) -> int:
    # extract number from pred_ans
    matches = re.findall(r"-?\d+(?:\.\d+)?", pred["answer"])
    result = matches[-1] if matches else None
    if result is None:
        return 0
    pred_ans = float(result)
    if isinstance(value_to_match, float):
        relative_error = abs(value_to_match) * 0.1
    else:
        relative_error = 1e-3
    return int(abs(pred_ans - value_to_match) < relative_error)


def location_matching(
    pred: Dict,
    location_fine_grained: List[str],
    location_coarse_grained: List[str] = [],
    fine_grained_score: float = 1.0,
    coarse_grained_score: float = 0.5,
) -> int:
    pred_ans = pred["answer"].lower()
    location_fine_grained = [
        location_fine_grained.lower() for location_fine_grained in location_fine_grained
    ]
    location_coarse_grained = [
        location_coarse_grained.lower()
        for location_coarse_grained in location_coarse_grained
    ]
    if any(
        location_fine_grained in pred_ans
        for location_fine_grained in location_fine_grained
    ):
        return fine_grained_score
    elif any(
        location_coarse_grained in pred_ans
        for location_coarse_grained in location_coarse_grained
    ):
        return coarse_grained_score
    return 0


@EVALUATORS.register_module()
class ROMEEvaluator(BaseEvaluator):
    def __init__(
        self,
        tracker_type,
        tracker_subtype=None,
        **kwargs,
    ):
        self.tracker_type = tracker_type
        self.tracker_subtype = tracker_subtype
        super().__init__(**kwargs)

    def get_score(self, gt: Dict, pred: Dict) -> Union[float, List[float]]:
        evaluator = gt["evaluator"]
        pred["raw_answer"] = pred["answer"]
        pred["answer"] = normalize_string(extract_answer(pred))
        registed_evaluator = set(
            [
                "key_items_matching",
                "choices_matching",
                "ordered_list_matching",
                "number_matching",
                "location_matching",
                "interval_matching",
                "multi_interval_matching",
            ]
        )
        if evaluator not in registed_evaluator:
            raise ValueError(f"Unsupported evaluator: {evaluator}")
        return eval(evaluator)(pred, **gt["evaluator_kwargs"])

    def get_score_by_llm(self, gt: Dict, pred: Dict) -> Tuple[str, int]:
        """Custom grading method"""
        prompt = (
            demo_prompt_score.replace("{{question}}", gt["question"])
            .replace("{{answer}}", gt["reference"])
            .replace("{{extracted_answer}}", pred["answer"])
        )

        message = self.llm_evaluator.build_message(query=prompt)
        compare_result_response = self.llm_evaluator.infer(
            chat_messages=message, temperature=0, top_p=1, seed=42
        )
        compare_result = compare_result_response.content.replace(
            "Judgement:", ""
        ).strip()
        # Extract the score from the custom evaluation result
        # The score should be after the last double newline (\n\n)
        if "\n\n" in compare_result:
            # Split by double newlines and get the last part
            parts = compare_result.split("\n\n")
            score_part = parts[-1].strip()

            # Extract numeric value from the score part
            import re

            score_match = re.search(r"(\d+(?:\.\d+)?)", score_part)
            if score_match:
                compare_result = score_match.group(1)
            else:
                # If no numeric value found, default to 0
                compare_result = "0"
        return compare_result_response.content, int(compare_result)

    def cal_accuracy(
        self, annotations: Dict, predictions: List[Dict], *args, **kwargs
    ) -> Dict:
        class ScoreTracker:
            def __init__(self):
                self.total_score = 0
                self.count = 0
                self.accuracy = 0
                self.subtypes = defaultdict(
                    lambda: [0, 0, 0]
                )  # [score_sum, count, accuracy]

            def update(self, score, sub_type):
                self.total_score += score
                self.count += 1
                self.subtypes[sub_type][0] += score
                self.subtypes[sub_type][1] += 1

        results = {}
        scores_by_type = defaultdict(ScoreTracker)
        for pred in predictions:
            question_id = str(pred["question_id"])
            gt = annotations[question_id]
            if self.use_llm_evaluator:
                judgement_response, score = self.get_score_by_llm(gt, pred)
            else:
                score = self.get_score(gt, pred)
            pred.update(gt)
            pred["correct"] = score
            pred["judgement_response"] = (
                judgement_response if self.use_llm_evaluator else None
            )
            # Update scores
            tracker = scores_by_type[pred[self.tracker_type]]
            if self.tracker_subtype is not None:
                tracker.update(score, pred[self.tracker_subtype])
            else:
                tracker.update(score, pred[self.tracker_type])
        # Calculate accuracy
        for tracker in scores_by_type.values():
            tracker.accuracy = round(tracker.total_score / tracker.count, 3)
            for sub_type in tracker.subtypes:
                tracker.subtypes[sub_type][2] = round(
                    tracker.subtypes[sub_type][0] / tracker.subtypes[sub_type][1], 3
                )
        final_score = sum(tracker.total_score for tracker in scores_by_type.values())
        results["final_score"] = [final_score, len(predictions)]
        results["accuracy"] = round(final_score / len(predictions) * 100, 3)

        # Convert ScoreTracker objects to the expected format
        for qtype, tracker in scores_by_type.items():
            results[qtype] = [
                tracker.total_score,
                tracker.count,
                tracker.accuracy,
                dict(tracker.subtypes),
            ]

        return results
